# 只有同意了EULA和PRIVACY协议才可以部署麦麦
# 配置以下的选项为true表示你同意了EULA和PRIVACY条款
# https://github.com/Mai-with-u/MaiBot/blob/main/EULA.md
# https://github.com/Mai-with-u/MaiBot/blob/main/PRIVACY.md
EULA_AGREE: false
PRIVACY_AGREE: false

# 预处理Job的配置
pre_processor:
  image:
    repository: # 默认 reg.mikumikumi.xyz/maibot/preprocessor
    tag: # 默认 0.12.2
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  nodeSelector: { }
  tolerations: [ ]

# 麦麦Adapter的部署配置
adapter:

  image:
    repository: # 默认 unclas/maimbot-adapter
    tag: # 默认 main-20260109183519
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # 配置adapter的napcat websocket service
  # adapter会启动一个websocket服务端，用于与napcat通信
  # 这里的选项可以帮助你自定义服务端口
  # ！！！默认不使用NodePort。如果通过NodePort将服务端口映射到公网可能会被恶意客户端连接，请自行使用中间件鉴权！！！
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的websocket端口映射到物理节点的端口
    port: 8095  # websocket监听端口ClusterIP的端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  persistence:
    config:  # 配置文件的存储卷
      storageClass:
      accessModes:
        - ReadWriteOnce
      size: 10Mi
    data:  # 数据的存储卷
      storageClass:
      accessModes:
        - ReadWriteOnce
      size: 1Gi

# 麦麦本体的部署配置
core:

  image:
    repository: # 默认 sengokucola/maibot
    tag: # 默认 0.12.2
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  persistence:
    config:  # 配置文件的存储卷
      storageClass:
      accessModes:
        - ReadWriteOnce
      size: 10Mi
    data:  # 数据的存储卷
      storageClass:
      accessModes:
        - ReadWriteOnce
      size: 10Gi

  webui:  # WebUI相关配置
    enabled: true  # 默认启用
    service:
      type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
      port: 8001  # 服务端口
      nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口
    ingress:
      enabled: false
      className: nginx
      annotations: { }
      host: maim.example.com  # 访问麦麦WebUI的域名
      path: /
      pathType: Prefix

  maim_message_api_server:
    enabled: false
    service:
      type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
      port: 8090  # 服务端口
      nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口


# 麦麦的运行统计看板配置
# 麦麦每隔一段时间会自动输出html格式的运行统计报告，此统计报告可以作为静态网页访问
# 此功能默认禁用。如果你认为报告可以被公开访问（报告包含联系人/群组名称、模型token花费信息等），则可以启用此功能
# 如果启用此功能，你也可以考虑使用中间件进行鉴权，保护隐私信息
statistics_dashboard:

  enabled: false  # 是否启用运行统计看板

  replicaCount: 1

  image:
    repository: # 默认 nginx
    tag: # 默认 latest
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 80  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口
  ingress:
    enabled: false
    className: nginx
    annotations: { }
    host: maim-statistics.example.com  # 访问运行统计看板的域名
    path: /
    pathType: Prefix

  persistence:
    storageClass:
    # 如果你希望运行统计看板服务与麦麦本体运行在不同的节点（多活部署），那么需要ReadWriteMany访问模式
    # 注意：ReadWriteMany特性需要存储类底层支持
    accessModes:
      - ReadWriteOnce
    size: 100Mi

# napcat的部署配置
# ！！！napcat部署完毕后，务必修改默认密码！！！
napcat:

  # 考虑到复用外部napcat实例的情况，napcat部署已被解耦
  # 如果你有外部部署的napcat，则可以修改下面的enabled为false，本次不会重复部署napcat
  # 如果没有外部部署的napcat，默认会捆绑部署napcat，不需要修改此项
  enabled: true

  image:
    repository: # 默认 mlikiowa/napcat-docker
    tag: # 默认 v4.10.15
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # napcat进程的权限，默认不是特权用户
  permission:
    uid: 1000
    gid: 1000

  # 配置napcat web面板的service
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 6099  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  # 配置napcat web面板的ingress
  ingress:
    enabled: false  # 是否启用
    className: nginx
    annotations: { }
    host: napcat.example.com  # 暴露napcat web面板使用的域名
    path: /
    pathType: Prefix

  persistence:
    storageClass:
    accessModes:
      - ReadWriteOnce
    size: 5Gi

# sqlite-web的部署配置
sqlite_web:

  # 通过sqlite-web可以在网页上操作麦麦的数据库，方便调试。不部署对麦麦的运行无影响
  # 默认不会捆绑部署sqlite-web，如果你需要部署，请修改下面的enabled为true
  # ！！！sqlite-web服务无鉴权，暴露在公网上十分危险，推荐使用集群ClusterIP内网访问！！！
  # ！！！如果一定要暴露在公网，请自行使用中间件鉴权！！！
  enabled: false

  image:
    repository: # 默认 coleifer/sqlite-web
    tag: # 默认 latest
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # 配置sqlite-web面板的service
  # ！！！默认不使用NodePort。如果使用NodePort暴露到公网，请自行使用中间件鉴权！！！
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 8080  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  # 配置sqlite-web面板的ingress
  # ！！！默认不使用ingress。如果使用ingress暴露到公网，请自行使用中间件鉴权！！！
  ingress:
    enabled: false  # 是否启用
    className: nginx
    annotations: { }
    host: maim-sqlite.example.com  # 暴露websocket使用的域名
    path: /
    pathType: Prefix

# 设置麦麦各部分组件的初始运行配置
# 考虑到配置文件的操作复杂性增加，k8s的适配复杂度也同步增加，且WebUI可以直接修改配置文件
# 自0.11.6-beta版本开始，各组件的配置不再存储于k8s的configmap中，而是直接存储于存储卷的实际文件中
# 从旧版本升级的用户，旧的configmap的配置会自动迁移到新的存储卷的配置文件中
# 此处的配置只在初次部署时或者指定覆盖时注入到MaiBot中
config:

  # 指定是否用下面的配置覆盖MaiBot现有的配置文件
  override_adapter_config: false
  override_core_bot_config: false
  override_core_model_config: false

  # adapter的config.toml
  adapter_config: |
    [inner]
    version = "0.1.3" # 版本号
    # 请勿修改版本号，除非你知道自己在做什么
    
    [nickname] # 现在没用
    nickname = ""
    
    [napcat_server] # Napcat连接的ws服务设置
    token = ""              # Napcat设定的访问令牌，若无则留空
    heartbeat_interval = 30 # 与Napcat设置的心跳相同（按秒计）
    
    [chat] # 黑白名单功能
    group_list_type = "whitelist" # 群组名单类型，可选为：whitelist, blacklist
    group_list = []               # 群组名单
    # 当group_list_type为whitelist时，只有群组名单中的群组可以聊天
    # 当group_list_type为blacklist时，群组名单中的任何群组无法聊天
    private_list_type = "whitelist" # 私聊名单类型，可选为：whitelist, blacklist
    private_list = []               # 私聊名单
    # 当private_list_type为whitelist时，只有私聊名单中的用户可以聊天
    # 当private_list_type为blacklist时，私聊名单中的任何用户无法聊天
    ban_user_id = []   # 全局禁止名单（全局禁止名单中的用户无法进行任何聊天）
    ban_qq_bot = false # 是否屏蔽QQ官方机器人
    enable_poke = true # 是否启用戳一戳功能
    
    [voice] # 发送语音设置
    use_tts = false # 是否使用tts语音（请确保你配置了tts并有对应的adapter）
    
    [forward] # 转发消息处理设置
    image_threshold = 3 # 图片数量阈值：转发消息中图片数量超过此值时使用占位符(避免麦麦VLM处理卡死)
    
    [debug]
    level = "INFO" # 日志等级（DEBUG, INFO, WARNING, ERROR, CRITICAL）

  # core的model_config.toml
  core_model_config: |
    [inner]
    version = "1.11.0"
    
    # 配置文件版本号迭代规则同bot_config.toml
    
    [[api_providers]] # API服务提供商（可以配置多个）
    name = "DeepSeek"                       # API服务商名称（可随意命名，在models的api-provider中需使用这个命名）
    base_url = "https://api.deepseek.com/v1" # API服务商的BaseURL
    api_key = "your-api-key-here"           # API密钥（请替换为实际的API密钥）
    client_type = "openai"                  # 请求客户端（可选，默认值为"openai"，使用gimini等Google系模型时请配置为"gemini"）
    max_retry = 2                           # 最大重试次数（单个模型API调用失败，最多重试的次数）
    timeout = 120                            # API请求超时时间（单位：秒）
    retry_interval = 10                     # 重试间隔时间（单位：秒）
    
    [[api_providers]] # 阿里 百炼 API服务商配置
    name = "BaiLian"
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    api_key = "your-bailian-key"
    client_type = "openai"
    max_retry = 2
    timeout = 120
    retry_interval = 5
    
    [[api_providers]] # 特殊：Google的Gimini使用特殊API，与OpenAI格式不兼容，需要配置client为"gemini"
    name = "Google"
    base_url = "https://generativelanguage.googleapis.com/v1beta"
    api_key = "your-google-api-key-1"
    client_type = "gemini"
    max_retry = 2
    timeout = 120
    retry_interval = 10
    
    [[api_providers]] # SiliconFlow的API服务商配置
    name = "SiliconFlow"
    base_url = "https://api.siliconflow.cn/v1"
    api_key = "your-siliconflow-api-key"
    client_type = "openai"
    max_retry = 3
    timeout = 120
    retry_interval = 5
    
    
    [[models]] # 模型（可以配置多个）
    model_identifier = "deepseek-chat" # 模型标识符（API服务商提供的模型标识符）
    name = "deepseek-v3"               # 模型名称（可随意命名，在后面中需使用这个命名）
    api_provider = "DeepSeek"          # API服务商名称（对应在api_providers中配置的服务商名称）
    price_in = 2.0                     # 输入价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
    price_out = 8.0                    # 输出价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
    # force_stream_mode = true          # 强制流式输出模式（若模型不支持非流式输出，请取消该注释，启用强制流式输出，若无该字段，默认值为false）
    
    [[models]]
    model_identifier = "deepseek-ai/DeepSeek-V3.2-Exp"
    name = "siliconflow-deepseek-v3.2"
    api_provider = "SiliconFlow"
    price_in = 2.0
    price_out = 3.0
    # temperature = 0.5    # 可选：为该模型单独指定温度，会覆盖任务配置中的温度
    # max_tokens = 4096    # 可选：为该模型单独指定最大token数，会覆盖任务配置中的max_tokens
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = false # 不启用思考
    
    
    [[models]]
    model_identifier = "deepseek-ai/DeepSeek-V3.2-Exp"
    name = "siliconflow-deepseek-v3.2-think"
    api_provider = "SiliconFlow"    
    price_in = 2.0
    price_out = 3.0
    # temperature = 0.7    # 可选：为该模型单独指定温度，会覆盖任务配置中的温度
    # max_tokens = 4096    # 可选：为该模型单独指定最大token数，会覆盖任务配置中的max_tokens
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = true # 启用思考
    
    
    [[models]]
    model_identifier = "Qwen/Qwen3-Next-80B-A3B-Instruct"
    name = "qwen3-next-80b"
    api_provider = "SiliconFlow"    
    price_in = 1.0
    price_out = 4.0
    
    [[models]]
    model_identifier = "zai-org/GLM-4.6"
    name = "siliconflow-glm-4.6"
    api_provider = "SiliconFlow"    
    price_in = 3.5
    price_out = 14.0
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = false # 不启用思考
    
    [[models]]
    model_identifier = "zai-org/GLM-4.6"
    name = "siliconflow-glm-4.6-think"
    api_provider = "SiliconFlow"    
    price_in = 3.5
    price_out = 14.0
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = true # 启用思考
    
    [[models]]
    model_identifier = "deepseek-ai/DeepSeek-R1"
    name = "siliconflow-deepseek-r1"
    api_provider = "SiliconFlow"
    price_in = 4.0
    price_out = 16.0
    
    
    [[models]]
    model_identifier = "Qwen/Qwen3-30B-A3B-Instruct-2507"
    name = "qwen3-30b"
    api_provider = "SiliconFlow"
    price_in = 0.7
    price_out = 2.8
    
    [[models]]
    model_identifier = "Qwen/Qwen3-VL-30B-A3B-Instruct"
    name = "qwen3-vl-30"
    api_provider = "SiliconFlow"
    price_in = 4.13
    price_out = 4.13
    
    [[models]]
    model_identifier = "FunAudioLLM/SenseVoiceSmall"
    name = "sensevoice-small"
    api_provider = "SiliconFlow"
    price_in = 0
    price_out = 0
    
    [[models]]
    model_identifier = "BAAI/bge-m3"
    name = "bge-m3"
    api_provider = "SiliconFlow"
    price_in = 0
    price_out = 0
    
    
    
    [model_task_config.utils] # 在麦麦的一些组件中使用的模型，例如表情包模块，取名模块，关系模块，麦麦的情绪变化等，是麦麦必须的模型
    model_list = ["siliconflow-deepseek-v3.2"] # 使用的模型列表，每个子项对应上面的模型名称(name)
    temperature = 0.2                        # 模型温度，新V3建议0.1-0.3
    max_tokens = 4096                         # 最大输出token数
    slow_threshold = 15.0                     # 慢请求阈值（秒），模型等待回复时间超过此值会输出警告日志
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.tool_use] #功能模型，需要使用支持工具调用的模型，请使用较快的小模型（调用量较大）
    model_list = ["qwen3-30b","qwen3-next-80b"]
    temperature = 0.7
    max_tokens = 1024
    slow_threshold = 10.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.replyer] # 首要回复模型，还用于表达方式学习
    model_list = ["siliconflow-deepseek-v3.2","siliconflow-deepseek-v3.2-think","siliconflow-glm-4.6","siliconflow-glm-4.6-think"]
    temperature = 0.3                        # 模型温度，新V3建议0.1-0.3
    max_tokens = 2048
    slow_threshold = 25.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.planner] #决策：负责决定麦麦该什么时候回复的模型
    model_list = ["siliconflow-deepseek-v3.2"]
    temperature = 0.3
    max_tokens = 800
    slow_threshold = 12.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.vlm] # 图像识别模型
    model_list = ["qwen3-vl-30"]
    max_tokens = 256
    slow_threshold = 15.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.voice] # 语音识别模型
    model_list = ["sensevoice-small"]
    slow_threshold = 12.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    # 嵌入模型
    [model_task_config.embedding]
    model_list = ["bge-m3"]
    slow_threshold = 5.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    # ------------LPMM知识库模型------------
    
    [model_task_config.lpmm_entity_extract] # 实体提取模型
    model_list = ["siliconflow-deepseek-v3.2"]
    temperature = 0.2
    max_tokens = 800
    slow_threshold = 20.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）
    
    [model_task_config.lpmm_rdf_build] # RDF构建模型
    model_list = ["siliconflow-deepseek-v3.2"]
    temperature = 0.2
    max_tokens = 800
    slow_threshold = 20.0
    selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

  # core的bot_config.toml
  core_bot_config: |
    [inner]
    version = "7.3.5"
    
    #----以下是给开发人员阅读的，如果你只是部署了麦麦，不需要阅读----
    # 如果你想要修改配置文件，请递增version的值
    # 如果新增项目，请阅读src/config/official_configs.py中的说明
    #
    # 版本格式：主版本号.次版本号.修订号，版本号递增规则如下：
    #     主版本号：MMC版本更新
    #     次版本号：配置文件内容大更新
    #     修订号：配置文件内容小更新
    #----以上是给开发人员阅读的，如果你只是部署了麦麦，不需要阅读----
    
    [bot]
    platform = "qq" 
    qq_account = "1145141919810" # 麦麦的QQ账号
    
    platforms = ["wx:114514","xx:1919810"] # 麦麦的其他平台账号
    
    nickname = "麦麦" # 麦麦的昵称
    alias_names = ["麦叠", "牢麦"] # 麦麦的别名
    
    [personality]
    # 建议100字以内，描述人格特质 和 身份特征
    personality = "是一个大二在读女大学生，现在正在上网和群友聊天，有时有点攻击性，有时比较温柔"  
    # アイデンティティがない 生まれないらららら
    # 描述麦麦说话的表达风格，表达习惯，如要修改，可以酌情新增内容，建议1-2行
    reply_style = "请不要刻意突出自身学科背景。可以参考贴吧，知乎和微博的回复风格。"
    
    # 多种回复风格，根据概率随机选择表达风格
    multiple_reply_style = [
        # "你的风格平淡但不失讽刺，很简短,很白话。可以参考贴吧，微博的回复风格。",
        # "用1-2个字进行回复",
        # "用1-2个符号进行回复",
        # "言辭凝練古雅，穿插《論語》經句卻不晦澀，以文言短句為基，輔以淺白語意，持長者溫和風範，全用繁體字表達，具先秦儒者談吐韻致。",
        # "带点翻译腔，但不要太长",
    ]
    
    # 使用多种回复风风格的概率
    multiple_probability = 0.3
    
    # 麦麦的说话规则和行为规则:
    plan_style = """1.思考**所有**的可用的action中的**每个动作**是否符合当下条件，如果动作使用条件符合聊天内容就使用
    2.如果相同的action已经被执行，请不要重复执行该action
    3.如果有人对你感到厌烦，请减少回复
    4.如果有人在追问你，或者话题没有说完，请你继续回复
    5.请分析哪些对话是和你说的，哪些是其他人之间的互动，不要误认为其他人之间的互动是和你说的"""
    
    # 多重人格，根据概率随机选择人格
    states = [
        "是一个女大学生，喜欢上网聊天，会刷小红书。" ,
        "是一个大二心理学生，会刷贴吧和中国知网。" ,
        "是一个赛博网友，最近很想吐槽人。" 
    ]
    
    # 使用多重人格的概率
    state_probability = 0.3
    
    # 麦麦识图规则，不建议修改
    visual_style = "请用中文描述这张图片的内容。如果有文字，请把文字描述概括出来，请留意其主题，直观感受，输出为一段平文本，最多30字，请注意不要分点，就输出一段文本"
    
    
    [expression]
    # 表达学习配置
    learning_list = [ # 表达学习配置列表，支持按聊天流配置
        ["", "enable", "enable", "enable"],  # 全局配置：使用表达，启用学习，启用jargon学习
        ["qq:1919810:group", "enable", "enable", "enable"],  # 特定群聊配置：使用表达，启用学习，启用jargon学习
        ["qq:114514:private", "enable", "disable", "disable"],  # 特定私聊配置：使用表达，禁用学习，禁用jargon学习
        # 格式说明：
        # 第一位: chat_stream_id，空字符串表示全局配置
        # 第二位: 是否使用学到的表达 ("enable"/"disable")
        # 第三位: 是否学习表达 ("enable"/"disable") 
        # 第四位: 是否启用jargon学习 ("enable"/"disable")
    ]
    
    expression_groups = [
        # ["*"], # 全局共享组：所有chat_id共享学习到的表达方式（取消注释以启用全局共享）
        ["qq:1919810:private","qq:114514:private","qq:1111111:group"], # 特定互通组，相同组的chat_id会共享学习到的表达方式
        # 格式说明：
        # ["*"] - 启用全局共享，所有聊天流共享表达方式
        # ["qq:123456:private","qq:654321:group"] - 特定互通组，组内chat_id共享表达方式
        # 注意：如果为群聊，则需要设置为group，如果设置为私聊，则需要设置为private
    ]
    
    expression_checked_only = true # 麦麦只会使用检查过的表达方式
    
    expression_self_reflect = true # 是否启用自动表达优化
    expression_auto_check_interval = 600 # 表达方式自动检查的间隔时间（单位：秒），默认值：600秒（10分钟）
    expression_auto_check_count = 20 # 每次自动检查时随机选取的表达方式数量，默认值：20条
    expression_auto_check_custom_criteria = [] # 表达方式自动检查的额外自定义评估标准，格式：["标准1", "标准2", "标准3", ...]，这些标准会被添加到评估提示词中，作为额外的评估要求
    
    expression_manual_reflect = false # 是否启用手动表达优化
    manual_reflect_operator_id = "" # 手动表达优化操作员ID，格式：platform:id:type (例如 "qq:123456:private" 或 "qq:654321:group")
    allow_reflect = [] # 允许进行表达反思的聊天流ID列表，格式：["qq:123456:private", "qq:654321:group", ...]，只有在此列表中的聊天流才会提出问题并跟踪。如果列表为空，则所有聊天流都可以进行表达反思（前提是 reflect = true）
    
    
    all_global_jargon = true # 是否开启全局黑话模式，注意，此功能关闭后，已经记录的全局黑话不会改变，需要手动删除
    enable_jargon_explanation = true # 是否在回复前尝试对上下文中的黑话进行解释（关闭可减少一次LLM调用，仅影响回复前的黑话匹配与解释，不影响黑话学习）
    jargon_mode = "planner" # 黑话解释来源模式，可选： "context"（使用上下文自动匹配黑话） 或 "planner"（仅使用Planner在reply动作中给出的unknown_words列表）
    
    
    
    [chat] # 麦麦的聊天设置
    talk_value = 1 # 聊天频率，越小越沉默，范围0-1
    mentioned_bot_reply = true # 是否启用提及必回复
    max_context_size = 30 # 上下文长度
    planner_smooth = 3 # 规划器平滑，增大数值会减小planner负荷，略微降低反应速度，推荐1-5，0为关闭，必须大于等于0
    think_mode = "dynamic" # 思考模式，可选：classic（默认浅度思考和回复）、deep（会进行比较长的，深度回复）、dynamic（动态选择两种模式）
    
    plan_reply_log_max_per_chat = 1024 # 每个聊天保存最大的Plan/Reply日志数量，超过此数量时会自动删除最老的日志
    
    llm_quote = false # 是否由llm执行引用
    
    enable_talk_value_rules = true # 是否启用动态发言频率规则
    
    # 动态发言频率规则：按时段/按chat_id调整 talk_value（优先匹配具体chat，再匹配全局）
    # 推荐格式（对象数组）：{ target="platform:id:type" 或 "", time="HH:MM-HH:MM", value=0.5 }
    # 说明:
    # - target 为空字符串表示全局；type 为 group/private，例如："qq:1919810:group" 或 "qq:114514:private"；
    # - 支持跨夜区间，例如 "23:00-02:00"；数值范围建议 0-1，如果 value 设置为0会自动转换为0.0001以避免除以零错误。
    talk_value_rules = [
        { target = "", time = "00:00-08:59", value = 0.8 },
        { target = "", time = "09:00-22:59", value = 1.0 },
        { target = "qq:1919810:group", time = "20:00-23:59", value = 0.6 },
        { target = "qq:114514:private", time = "00:00-23:59", value = 0.3 },
    ]
    
    [memory]
    max_agent_iterations = 5 # 记忆思考深度（最低为1）
    agent_timeout_seconds = 180.0 # 最长回忆时间（秒）
    global_memory = false # 是否允许记忆检索进行全局查询
    global_memory_blacklist = [
    
    ] # 全局记忆黑名单，当启用全局记忆时，不将特定聊天流纳入检索。格式: ["platform:id:type", ...]，例如: ["qq:1919810:private", "qq:114514:group"]
    planner_question = true # 是否使用 Planner 提供的 question 作为记忆检索问题。开启后，当 Planner 在 reply 动作中提供了 question 时，直接使用该问题进行记忆检索，跳过 LLM 生成问题的步骤；关闭后沿用旧模式，使用 LLM 生成问题
    
    [dream]
    interval_minutes = 60 # 做梦时间间隔（分钟），默认30分钟
    max_iterations = 20 # 做梦最大轮次，默认20轮
    first_delay_seconds = 1800 # 程序启动后首次做梦前的延迟时间（秒），默认60秒
    
    # 做梦结果推送目标，格式为 "platform:user_id"
    # 例如: "qq:123456" 表示在做梦结束后，将梦境文本额外发送给该QQ私聊用户。
    # 为空字符串时不推送。
    dream_send = ""
    
    dream_visible = false # 做梦结果是否存储到上下文，True: 将梦境发送给配置的用户后，也会存储到聊天上下文中，在后续对话中可见；False: 仅发送梦境但不存储，不在后续对话上下文中出现
    # 做梦时间段配置，格式：["HH:MM-HH:MM", ...]
    # 如果列表为空，则表示全天允许做梦。
    # 如果配置了时间段，则只有在这些时间段内才会实际执行做梦流程。
    # 时间段外，调度器仍会按间隔检查，但不会进入做梦流程。
    # 支持跨夜区间，例如 "23:00-02:00" 表示从23:00到次日02:00。
    # 示例：
    dream_time_ranges = [
        # "09:00-22:00",      # 白天允许做梦
        "23:00-10:00",      # 跨夜时间段（23:00到次日10:00）
    ]
    # dream_time_ranges = []
    
    [tool]
    enable_tool = true # 是否启用工具
    
    
    [emoji]
    emoji_chance = 0.4 # 麦麦激活表情包动作的概率
    max_reg_num = 100 # 表情包最大注册数量
    do_replace = true # 开启则在达到最大数量时删除（替换）表情包，关闭则达到最大数量时不会继续收集表情包
    check_interval = 10 # 检查表情包（注册，破损，删除）的时间间隔(分钟)
    steal_emoji = true # 是否偷取表情包，让麦麦可以将一些表情包据为己有
    content_filtration = false  # 是否启用表情包过滤，只有符合该要求的表情包才会被保存
    filtration_prompt = "符合公序良俗" # 表情包过滤要求，只有符合该要求的表情包才会被保存
    
    [voice]
    enable_asr = false # 是否启用语音识别，启用后麦麦可以识别语音消息，启用该功能需要配置语音识别模型[model_task_config.voice]
    
    [message_receive]
    # 以下是消息过滤，可以根据规则过滤特定消息，将不会读取这些消息
    ban_words = [
        # "403","张三"
        ]
    
    ban_msgs_regex = [
        # 需要过滤的消息（原始消息）匹配的正则表达式，匹配到的消息将被过滤，若不了解正则表达式请勿修改
        # "https?://[^\\s]+", # 匹配https链接
        # "\\d{4}-\\d{2}-\\d{2}", # 匹配日期
    ]
    
    
    [lpmm_knowledge] # lpmm知识库配置
    enable = false # 是否启用lpmm知识库
    lpmm_mode = "agent"
    # 可选择classic传统模式/agent 模式，结合新的记忆一同使用
    rag_synonym_search_top_k = 10 # 同义检索TopK
    rag_synonym_threshold = 0.8 # 同义阈值，相似度高于该值的关系会被当作同义词
    info_extraction_workers = 3 # 实体抽取同时执行线程数，非Pro模型不要设置超过5
    qa_relation_search_top_k = 10 # 关系检索TopK
    qa_relation_threshold = 0.5 # 关系阈值，相似度高于该值的关系会被认为是相关关系
    qa_paragraph_search_top_k = 1000 # 段落检索TopK（不能过小，可能影响搜索结果）
    qa_paragraph_node_weight = 0.05 # 段落节点权重（在图搜索&PPR计算中的权重，当搜索仅使用DPR时，此参数不起作用）
    qa_ent_filter_top_k = 10 # 实体过滤TopK
    qa_ppr_damping = 0.8 # PPR阻尼系数
    qa_res_top_k = 3 # 最终提供段落TopK
    embedding_dimension = 1024 # 嵌入向量维度,输出维度一致
    # 性能与降级参数（低配机器可下调）
    # 低配机器参考：单/双核或内存≤4GB（如轻量云主机/云函数/开发板），建议先关闭PPR并降低并发
    max_embedding_workers = 3 # 嵌入/抽取并发线程数
    embedding_chunk_size = 4 # 每批嵌入的条数
    max_synonym_entities = 2000 # 同义边参与的实体数上限，超限则跳过
    enable_ppr = true # 是否启用PPR，低配机器可关闭
    
    [keyword_reaction]
    keyword_rules = [
        { keywords = ["人机", "bot", "机器", "入机", "robot", "机器人", "ai", "AI"], reaction = "有人说你是人机或者机器人，否定这一事实，攻击或反问他。注意，只有认为你是机器人才回复，否则不要否认" },
        { keywords = ["测试关键词回复", "test"], reaction = "回答测试成功" },
        # { keywords = ["你好", "hello"], reaction = "你好，有什么可以帮你？" }    
        # 在此处添加更多规则，格式同上
    ]
    
    regex_rules = [
        { regex = ["^(?P<n>\\S{1,20})是这样的$"], reaction = "请按照以下模板造句：[n]是这样的，xx只要xx就可以，可是[n]要考虑的事情就很多了，比如什么时候xx，什么时候xx，什么时候xx。（请自由发挥替换xx部分，只需保持句式结构，同时表达一种将[n]过度重视的反讽意味）" },
        { regex = ["(?:[^，。.\\s]+，\\s*)?我(?:也)?[没沒]要求你\\s*(?P<action>.+?)[.。,，]?$"],reaction = "进行一次“八岐大蛇定型文”补完。暂时忽略回复风格，进入“粉絲吐槽”模式。请严格按照以下“填空模板”，围绕“[action]”进行创作，使其充满滑稽的夸张感和天马行空的联想：我没要求你[action]，我不是(与action关联的、宽容的角色)。可是，(与action相关的具体负面行为)是什么意思？你的(与action相关的某种观念)怎么了？你才(与action相关的某个状态或时间点)吧？再这样下去，你(一个中期的、等比级数式的滑稽推演)，(一个后期的、等比级数式的滑稽推演)，最后就变成(一个与主题相关的、夸张的最终形态)了。作为(与最终形态相关的、克星或权威身份)，我可能得(对你执行一个天罚般的行动)。真的。"}
    ]
    
    [response_post_process]
    enable_response_post_process = true # 是否启用回复后处理，包括错别字生成器，回复分割器
    
    [chinese_typo]
    enable = true # 是否启用中文错别字生成器
    error_rate=0.01 # 单字替换概率
    min_freq=9 # 最小字频阈值
    tone_error_rate=0.1 # 声调错误概率
    word_replace_rate=0.006 # 整词替换概率
    
    [response_splitter]
    enable = true # 是否启用回复分割器
    max_length = 512 # 回复允许的最大长度
    max_sentence_num = 8 # 回复允许的最大句子数
    enable_kaomoji_protection = false # 是否启用颜文字保护
    enable_overflow_return_all = false # 是否在句子数量超出回复允许的最大句子数时一次性返回全部内容
    
    [log]
    date_style = "m-d H:i:s" # 日期格式
    log_level_style = "lite" # 日志级别样式,可选FULL，compact，lite
    color_text = "full" # 日志文本颜色，可选none，title，full
    log_level = "INFO" # 全局日志级别（向下兼容，优先级低于下面的分别设置）
    console_log_level = "INFO" # 控制台日志级别，可选: DEBUG, INFO, WARNING, ERROR, CRITICAL
    file_log_level = "DEBUG" # 文件日志级别，可选: DEBUG, INFO, WARNING, ERROR, CRITICAL
    
    # 第三方库日志控制
    suppress_libraries = ["faiss","httpx", "urllib3", "asyncio", "websockets", "httpcore", "requests", "peewee", "openai","uvicorn","jieba"] # 完全屏蔽的库
    library_log_levels = { aiohttp = "WARNING"} # 设置特定库的日志级别
    
    [debug]
    show_prompt = false # 是否显示prompt
    show_replyer_prompt = false # 是否显示回复器prompt
    show_replyer_reasoning = false # 是否显示回复器推理
    show_jargon_prompt = false # 是否显示jargon相关提示词
    show_memory_prompt = false # 是否显示记忆检索相关提示词
    show_planner_prompt = false # 是否显示planner的prompt和原始返回结果
    show_lpmm_paragraph = false # 是否显示lpmm找到的相关文段日志
    
    [maim_message]
    auth_token = [] # 认证令牌，用于旧版API验证，为空则不启用验证
    
    # 新版API Server配置（额外监听端口）
    api_server_use_wss = false # 新版API Server是否启用WSS
    api_server_cert_file = "" # 新版API Server SSL证书文件路径
    api_server_key_file = "" # 新版API Server SSL密钥文件路径
    api_server_allowed_api_keys = [] # 新版API Server允许的API Key列表，为空则允许所有连接
    
    [telemetry] #发送统计信息，主要是看全球有多少只麦麦
    enable = true
    
    [webui] # WebUI 独立服务器配置
    mode = "production" # 模式: development(开发) 或 production(生产)
    
    # 防爬虫配置
    anti_crawler_mode = "loose" # 防爬虫模式: false(禁用) / strict(严格) / loose(宽松) / basic(基础-只记录不阻止)
    trusted_proxies = "" # 信任的代理IP列表（逗号分隔），只有来自这些IP的X-Forwarded-For才被信任
                         # 示例: 127.0.0.1,192.168.1.1,172.17.0.1
    trust_xff = false # 是否启用X-Forwarded-For代理解析（默认false）
                      # 启用后，仍要求直连IP在trusted_proxies中才会信任XFF头
    secure_cookie = false # 是否启用安全Cookie（仅通过HTTPS传输，默认false）
    
    [experimental] #实验性功能
    # 麦麦私聊的说话规则，行为风格（实验性功能）
    private_plan_style = """
    1.思考**所有**的可用的action中的**每个动作**是否符合当下条件，如果动作使用条件符合聊天内容就使用
    2.如果相同的内容已经被执行，请不要重复执行
    3.某句话如果已经被回复过，不要重复回复"""

    # 为指定聊天添加额外的prompt配置
    # 格式: ["platform:id:type:prompt内容", ...]
    # 示例:
    # chat_prompts = [
    #     "qq:114514:group:这是一个摄影群，你精通摄影知识",
    #     "qq:19198:group:这是一个二次元交流群",
    #     "qq:114514:private:这是你与好朋友的私聊"
    # ]
    chat_prompts = []
    
    
    # 此系统暂时移除，无效配置
    [relationship]
    enable_relationship = true # 是否启用关系系统
