[inner]
version = "1.11.0"

# 配置文件版本号迭代规则同bot_config.toml

[[api_providers]] # API服务提供商（可以配置多个）
name = "DeepSeek"                       # API服务商名称（可随意命名，在models的api-provider中需使用这个命名）
base_url = "https://api.deepseek.com/v1" # API服务商的BaseURL
api_key = "your-api-key-here"           # API密钥（请替换为实际的API密钥）
client_type = "openai"                  # 请求客户端（可选，默认值为"openai"，使用gimini等Google系模型时请配置为"gemini"）
max_retry = 2                           # 最大重试次数（单个模型API调用失败，最多重试的次数）
timeout = 120                            # API请求超时时间（单位：秒）
retry_interval = 10                     # 重试间隔时间（单位：秒）

[[api_providers]] # 阿里 百炼 API服务商配置
name = "BaiLian"
base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
api_key = "your-bailian-key"
client_type = "openai"
max_retry = 2
timeout = 120
retry_interval = 5

[[api_providers]] # 特殊：Google的Gimini使用特殊API，与OpenAI格式不兼容，需要配置client为"gemini"
name = "Google"
base_url = "https://generativelanguage.googleapis.com/v1beta"
api_key = "your-google-api-key-1"
client_type = "gemini"
max_retry = 2
timeout = 120
retry_interval = 10

[[api_providers]] # SiliconFlow的API服务商配置
name = "SiliconFlow"
base_url = "https://api.siliconflow.cn/v1"
api_key = "your-siliconflow-api-key"
client_type = "openai"
max_retry = 3
timeout = 120
retry_interval = 5


[[models]] # 模型（可以配置多个）
model_identifier = "deepseek-chat" # 模型标识符（API服务商提供的模型标识符）
name = "deepseek-v3"               # 模型名称（可随意命名，在后面中需使用这个命名）
api_provider = "DeepSeek"          # API服务商名称（对应在api_providers中配置的服务商名称）
price_in = 2.0                     # 输入价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
price_out = 8.0                    # 输出价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
# force_stream_mode = true          # 强制流式输出模式（若模型不支持非流式输出，请取消该注释，启用强制流式输出，若无该字段，默认值为false）

[[models]]
model_identifier = "deepseek-ai/DeepSeek-V3.2-Exp"
name = "siliconflow-deepseek-v3.2"
api_provider = "SiliconFlow"
price_in = 2.0
price_out = 3.0
# temperature = 0.5    # 可选：为该模型单独指定温度，会覆盖任务配置中的温度
# max_tokens = 4096    # 可选：为该模型单独指定最大token数，会覆盖任务配置中的max_tokens
[models.extra_params] # 可选的额外参数配置
enable_thinking = false # 不启用思考


[[models]]
model_identifier = "deepseek-ai/DeepSeek-V3.2-Exp"
name = "siliconflow-deepseek-v3.2-think"
api_provider = "SiliconFlow"    
price_in = 2.0
price_out = 3.0
# temperature = 0.7    # 可选：为该模型单独指定温度，会覆盖任务配置中的温度
# max_tokens = 4096    # 可选：为该模型单独指定最大token数，会覆盖任务配置中的max_tokens
[models.extra_params] # 可选的额外参数配置
enable_thinking = true # 启用思考


[[models]]
model_identifier = "Qwen/Qwen3-Next-80B-A3B-Instruct"
name = "qwen3-next-80b"
api_provider = "SiliconFlow"    
price_in = 1.0
price_out = 4.0

[[models]]
model_identifier = "zai-org/GLM-4.6"
name = "siliconflow-glm-4.6"
api_provider = "SiliconFlow"    
price_in = 3.5
price_out = 14.0
[models.extra_params] # 可选的额外参数配置
enable_thinking = false # 不启用思考

[[models]]
model_identifier = "zai-org/GLM-4.6"
name = "siliconflow-glm-4.6-think"
api_provider = "SiliconFlow"    
price_in = 3.5
price_out = 14.0
[models.extra_params] # 可选的额外参数配置
enable_thinking = true # 启用思考

[[models]]
model_identifier = "deepseek-ai/DeepSeek-R1"
name = "siliconflow-deepseek-r1"
api_provider = "SiliconFlow"
price_in = 4.0
price_out = 16.0


[[models]]
model_identifier = "Qwen/Qwen3-30B-A3B-Instruct-2507"
name = "qwen3-30b"
api_provider = "SiliconFlow"
price_in = 0.7
price_out = 2.8

[[models]]
model_identifier = "Qwen/Qwen3-VL-30B-A3B-Instruct"
name = "qwen3-vl-30"
api_provider = "SiliconFlow"
price_in = 4.13
price_out = 4.13

[[models]]
model_identifier = "FunAudioLLM/SenseVoiceSmall"
name = "sensevoice-small"
api_provider = "SiliconFlow"
price_in = 0
price_out = 0

[[models]]
model_identifier = "BAAI/bge-m3"
name = "bge-m3"
api_provider = "SiliconFlow"
price_in = 0
price_out = 0



[model_task_config.utils] # 在麦麦的一些组件中使用的模型，例如表情包模块，取名模块，关系模块，麦麦的情绪变化等，是麦麦必须的模型
model_list = ["siliconflow-deepseek-v3.2"] # 使用的模型列表，每个子项对应上面的模型名称(name)
temperature = 0.2                        # 模型温度，新V3建议0.1-0.3
max_tokens = 4096                         # 最大输出token数
slow_threshold = 15.0                     # 慢请求阈值（秒），模型等待回复时间超过此值会输出警告日志
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.tool_use] #功能模型，需要使用支持工具调用的模型，请使用较快的小模型（调用量较大）
model_list = ["qwen3-30b","qwen3-next-80b"]
temperature = 0.7
max_tokens = 1024
slow_threshold = 10.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.replyer] # 首要回复模型，还用于表达方式学习
model_list = ["siliconflow-deepseek-v3.2","siliconflow-deepseek-v3.2-think","siliconflow-glm-4.6","siliconflow-glm-4.6-think"]
temperature = 0.3                        # 模型温度，新V3建议0.1-0.3
max_tokens = 2048
slow_threshold = 25.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.planner] #决策：负责决定麦麦该什么时候回复的模型
model_list = ["siliconflow-deepseek-v3.2"]
temperature = 0.3
max_tokens = 800
slow_threshold = 12.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.vlm] # 图像识别模型
model_list = ["qwen3-vl-30"]
max_tokens = 256
slow_threshold = 15.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.voice] # 语音识别模型
model_list = ["sensevoice-small"]
slow_threshold = 12.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

# 嵌入模型
[model_task_config.embedding]
model_list = ["bge-m3"]
slow_threshold = 5.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

# ------------LPMM知识库模型------------

[model_task_config.lpmm_entity_extract] # 实体提取模型
model_list = ["siliconflow-deepseek-v3.2"]
temperature = 0.2
max_tokens = 800
slow_threshold = 20.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）

[model_task_config.lpmm_rdf_build] # RDF构建模型
model_list = ["siliconflow-deepseek-v3.2"]
temperature = 0.2
max_tokens = 800
slow_threshold = 20.0
selection_strategy = "random"           # 模型选择策略：balance（负载均衡）或 random（随机选择）